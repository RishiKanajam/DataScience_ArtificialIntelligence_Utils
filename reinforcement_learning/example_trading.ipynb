{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Trading example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Links: [Stock Data (YahooFinance)](https://finance.yahoo.com/chart/NVDA?showOptin=1#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.KAjCAoMjAsQywxLjk1LG1hLHkpIiwiZGlzcGxheSI6IuKAjEJvbGxpbmdlciBCYW5kc.KAjCAoMjAsQywxLjk1LG1hLHkpIn0sIm91dHB1dHMiOnsiQm9sbGluZ2VyIEJhbmRzIFRvcCI6IiMyYmJjZmYiLCJCb2xsaW5nZXIgQmFuZHMgTWVkaWFuIjoiIzA3ODBlYiIsIkJvbGxpbmdlciBCYW5kcyBCb3R0b20iOiIjMmJiY2ZmIn0sInBhbmVsIjoiY2hhcnQiLCJwYXJhbWV0ZXJzIjp7ImNoYXJ0TmFtZSI6ImNoYXJ0IiwicGFuZWxOYW1lIjoiY2hhcnQifX0sIuKAjG1h4oCMICgxMCxDLG1hLDApIjp7InR5cGUiOiJtYSIsImlucHV0cyI6eyJQZXJpb2QiOiIxMCIsIkZpZWxkIjoiQ2xvc2UiLCJUeXBlIjoic2ltcGxlIiwiT2Zmc2V0IjowLCJpZCI6IuKAjG1h4oCMICgxMCxDLG1hLDApIiwiZGlzcGxheSI6IuKAjG1h4oCMICgxMCxDLG1hLDApIn0sIm91dHB1dHMiOnsiTUEiOiIjMDBiMDYxIn0sInBhbmVsIjoiY2hhcnQiLCJwYXJhbWV0ZXJzIjp7ImNoYXJ0TmFtZSI6ImNoYXJ0IiwicGFuZWxOYW1lIjoiY2hhcnQifX19fQ--)  |  [Article (TowardsDataScience)]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import pckgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## for data\n",
    "import yfinance as yf  #0.2.22\n",
    "import numpy as np  #1.24.4\n",
    "import pandas as pd  #2.0.3\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import random\n",
    "\n",
    "## for plotting\n",
    "import plotly.graph_objects as go  #5.15.0\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "## for env\n",
    "#import gymnasium as gym  #0.28.1\n",
    "#from gymnasium import spaces\n",
    "\n",
    "## for rl\n",
    "from tensorflow.keras import models, layers, optimizers  #2.13.0\n",
    "from stable_baselines3 import A2C  #2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"NVDA\"\n",
    "\n",
    "data = yf.Ticker(x).history(period=\"1y\")  #1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max\n",
    "data.index = data.index.strftime(\"%Y-%m-%d\")\n",
    "data[\"t\"] = range(len(data))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create env\n",
    "class NewEnv():\n",
    "    \n",
    "    def __init__(self, data, fee=0.1):\n",
    "        ## create states space (dataframe)\n",
    "        self.data = data\n",
    "        self.state_space = self.create_state_space(self.data)\n",
    "        self.state_space_n = self.state_space.shape[1]\n",
    "        self.fee = fee\n",
    "        ## create action space (dict)\n",
    "        self.action_space = {\"buy\":1, \"hold\":0, \"sell\":-1}\n",
    "        self.action_space_n = len(self.action_space)\n",
    "        ## reset env when you start\n",
    "        self.state = self.reset(verbose=1, from_start=True)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def create_state_space(data, history=1, features=[\"High\",\"Low\",\"Close\",\"Volume\"]):\n",
    "        ## main price to trade\n",
    "        dtf = data.rename(columns={\"Open\":\"y\"})\n",
    "        dtf[\"y_perc\"] = dtf[\"y\"].pct_change(periods=1, fill_method='pad')\n",
    "        dtf = dtf[[\"t\",\"y\",\"y_perc\"] + features]\n",
    "        ## other prices\n",
    "        for i in range(history):\n",
    "            for x in features:\n",
    "                dtf[f\"{x}_{i+1}\"] = dtf[x].shift(i+1)\n",
    "        dtf = dtf.dropna(axis=0).set_index(\"t\")\n",
    "        return dtf\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    1.Reset the state of the env to an initial state or to a random point within the data.\n",
    "    '''\n",
    "    def reset(self, from_start=True, verbose=0):\n",
    "        self.quantity, self.position, self.cum_reward = 0, 0, 0\n",
    "        self.info = []\n",
    "        self.t = self.state_space.index[0] if from_start is True else np.random.choice(self.state_space.index[:-1])\n",
    "        self.state = self.state_space.loc[self.t]\n",
    "        if verbose != 0:\n",
    "            print(\"--- env reset | start from:\", self.t, \"---\")\n",
    "            print(\"current state:\\n\", self.state)\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    2. Compute reward after action.\n",
    "    '''\n",
    "    def calculate_reward(self, trade):\n",
    "        price = self.state_space[\"y\"][self.t]\n",
    "        reward = 0\n",
    "        ## buy (reward=fee)\n",
    "        if trade > 0:\n",
    "            new_quantity = self.quantity + trade\n",
    "            self.position = (self.position*self.quantity/new_quantity) + (price*trade/new_quantity)\n",
    "            self.quantity = new_quantity\n",
    "            reward = -self.fee\n",
    "        ## sell only if there is an open position (reward=return)\n",
    "        elif (trade < 0) & (self.quantity > 0):\n",
    "            reward = (price*trade + self.position)/self.position\n",
    "            self.position, self.quantity = 0, 0\n",
    "        return reward\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    3. Decide whether the episode is done after action.\n",
    "    '''\n",
    "    def is_done(self):\n",
    "        done = True if self.t == self.state_space.index[-1] else False\n",
    "        return done\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    4. Log info after action.\n",
    "    '''\n",
    "    def get_info(self, action, reward, done):\n",
    "        self.info.append({\n",
    "            \"t\":self.t, \"action\":action, \"quantity\":self.quantity, \n",
    "            \"reward\":reward, \"cum_reward\":self.cum_reward, \"done\":done\n",
    "        })\n",
    "        return pd.DataFrame(self.info)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Execute one step based on action.\n",
    "    '''\n",
    "    def step(self, action):\n",
    "        trade = 1*action  #<-- fixed volume=1 for now\n",
    "        reward = self.calculate_reward(trade)\n",
    "        self.cum_reward += reward\n",
    "        done = self.is_done()\n",
    "        if done is False:\n",
    "            self.t += 1\n",
    "            self.state = self.state_space.loc[self.t]\n",
    "        info = self.get_info(action, reward, done)\n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Rendering.\n",
    "    '''\n",
    "    def render(self):\n",
    "        fig = make_subplots(rows=4, cols=1, shared_xaxes=True, \n",
    "                            row_titles=['Play','Q','R','Tot'], \n",
    "                            row_heights=[0.55, 0.15, 0.15, 0.15])\n",
    "        fig.update_layout(plot_bgcolor='white', yaxis={\"gridcolor\":\"lightgrey\"}, hovermode=\"x\", \n",
    "                          showlegend=False, xaxis_rangeslider_visible=False, height=600)\n",
    "\n",
    "        ## price\n",
    "        fig.append_trace(go.Candlestick(x=self.data['t'], open=self.data['Open'], high=self.data['High'], \n",
    "                                        low=self.data['Low'], close=self.data['Close']),\n",
    "                         row=1, col=1)\n",
    "        #fig.append_trace(go.Scatter(x=self.data[\"t\"], y=self.data[\"y\"], name='y', mode='lines', line={\"color\":\"black\"}),\n",
    "        #                 row=1, col=1)\n",
    "        \n",
    "        ## other info\n",
    "        if len(self.info) > 0:\n",
    "            info = pd.DataFrame(self.info)\n",
    "            \n",
    "            ### quantity\n",
    "            fig.append_trace(go.Scatter(x=info[\"t\"], y=info[\"quantity\"], name='Quantity', \n",
    "                                        mode='lines', line={\"color\":\"black\", \"width\":0.7}), \n",
    "                             row=2, col=1)\n",
    "            ### rewards\n",
    "            fig.append_trace(go.Scatter(x=info[\"t\"], y=info[\"reward\"], name='Reward', \n",
    "                                        mode='lines', line={\"color\":\"black\", \"width\":0.7}), \n",
    "                             row=3, col=1)\n",
    "            ### cumultated rewards\n",
    "            fig.append_trace(go.Scatter(x=info[\"t\"], y=info[\"cum_reward\"], name='Total Reward', \n",
    "                                        mode='lines', line={\"color\":\"black\", \"width\":0.7}), \n",
    "                             row=4, col=1)\n",
    "            \n",
    "            ### actions\n",
    "            buys = info[info[\"action\"]==1]\n",
    "            for _,i in buys.iterrows():\n",
    "                #fig.add_vline(x=i[\"t\"], line_width=1, line_dash=\"dot\", line_color=\"blue\")\n",
    "                fig.add_shape({\"x0\":i[\"t\"], \"x1\":i[\"t\"], \"y0\":self.data[\"Low\"].min(), \"y1\":self.data[\"High\"].max(), \n",
    "                               \"type\":\"line\", \"line\":{\"width\":0.5,\"dash\":\"dot\",\"color\":\"green\"} })\n",
    "                \n",
    "            sells = info[(info[\"action\"]==-1) & (info[\"reward\"]!=0)]\n",
    "            for _,i in sells.iterrows():\n",
    "                #fig.add_vline(x=i[\"t\"], line_width=1, line_dash=\"dot\", line_color=\"orange\")\n",
    "                fig.add_shape({\"x0\":i[\"t\"], \"x1\":i[\"t\"], \"y0\":self.data[\"Low\"].min(), \"y1\":self.data[\"High\"].max(), \n",
    "                               \"type\":\"line\", \"line\":{\"width\":0.5,\"dash\":\"dot\",\"color\":\"red\"} })\n",
    "            \n",
    "            fig.add_hline(y=0, line_width=0.3, line_dash=\"dash\", line_color=\"gray\", row=[2,3,4])\n",
    "        \n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = NewEnv(data, fee=0.1)\n",
    "print(\"\\n-> state_space_n =\", env.state_space_n)\n",
    "print(\"-> action_space_n =\", env.action_space_n, \"| actions =\", env.action_space)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = env.reset(from_start=True, verbose=1)\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.random.choice( list(env.action_space.values()) )  #<--random action\n",
    "    state, reward, done, info = env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2 - Q Learning (descrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = pd.DataFrame(np.zeros(shape=(len(env.state_space), len(env.action_space))), \n",
    "                 columns=env.action_space.keys(), \n",
    "                 index=env.state_space.index)\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = pd.DataFrame(np.zeros(shape=(len(env.state_space), len(env.action_space))), \n",
    "                 columns=env.action_space.keys(), \n",
    "                 index=env.state_space.index)\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_action(Q, current_state, actions_space, explore_rate=0.5):            \n",
    "    ## explore: select a random action\n",
    "    if np.random.uniform(0,1) < explore_rate:\n",
    "        action = np.random.choice(actions_space)\n",
    "    ## exploit: select the action with max q_value\n",
    "    else:\n",
    "        current_qs = Q.loc[current_state]\n",
    "        action = np.argmax(current_qs)\n",
    "    return action\n",
    "\n",
    "\n",
    "def decay_exploration(explore_rate, min_eploration=0.001, explore_decay=0.99):\n",
    "    ## reduce the exploration rate \n",
    "    if explore_rate > min_eploration:\n",
    "        epsilon_decayed = explore_rate * explore_decay\n",
    "        explore_rate = max(min_eploration, epsilon_decayed)\n",
    "    ## keep minimun exporation\n",
    "    else:\n",
    "        explore_rate = explore_rate\n",
    "    return explore_rate\n",
    "\n",
    "\n",
    "def update_Q(Q, current_state, new_state, reward, done, discount_rate=0.8, learning_rate=0.1):\n",
    "    ## comupte new_q_value\n",
    "    if not done:\n",
    "        max_q_value = np.max(Q.loc[new_state])\n",
    "        current_q_value = Q.loc[current_state, new_state]\n",
    "        new_q_value = ((1-learning_rate)*current_q_value)+(learning_rate*(reward+(discount_rate*max_q_value)))\n",
    "    ## reward for reaching the goal\n",
    "    else:\n",
    "        new_q_value = 100\n",
    "    ## update\n",
    "    Q.loc[current_state, new_state] = new_q_value\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 1\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    current_state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        current_state = env.t\n",
    "        action = next_action(Q, env.t, actions_space=[1,0,-1], explore_rate=0.5)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        new_state = env.t\n",
    "        print(\"old state:\", current_state, \"new state:\", new_state)\n",
    "        \n",
    "        Q = update_Q(Q, current_state, new_state, reward, done, discount_rate=0.8, learning_rate=0.1)\n",
    "        explore_rate = decay_exploration(explore_rate, min_eploration=0.001, explore_decay=0.99)\n",
    "        current_state = new_state\n",
    "        break\n",
    "        \n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - DQN (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-> state_space_n =\", env.state_space_n)\n",
    "print(\"-> action_space_n =\", env.action_space_n)\n",
    "action_space = list(env.action_space.values())\n",
    "print(\"  \", action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create\n",
    "model = models.Sequential(name=\"DQN\", layers=[\n",
    "    ### layer input\n",
    "    layers.Dense(name=\"x_in\", input_dim=env.state_space_n, units=64, activation='relu'),\n",
    "    ### hidden layer 1\n",
    "    layers.Dense(name=\"h_1\", units=32, activation='relu'),\n",
    "    layers.BatchNormalization(name=\"norm_1\"),\n",
    "    layers.Dropout(name=\"drop_1\", rate=0.2),\n",
    "    ### hidden layer 2\n",
    "    layers.Dense(name=\"h_2\", units=8, activation='relu'),\n",
    "    layers.BatchNormalization(name=\"norm_2\"),\n",
    "    layers.Dropout(name=\"drop_2\", rate=0.2),\n",
    "    ### layer output\n",
    "    layers.Dense(name=\"y_out\", units=env.action_space_n, activation='linear')\n",
    "])\n",
    "## compile\n",
    "model.compile(loss=\"mse\", optimizer=optimizers.Adam(learning_rate=0.01))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(state):\n",
    "    array = state.values\n",
    "    state_preprocessed = np.reshape(array, [1,array.shape[0]])\n",
    "    return state_preprocessed\n",
    "\n",
    "\n",
    "def next_action(model, current_state, actions_space, explore_rate=1):            \n",
    "    ## explore: select a random action\n",
    "    if np.random.uniform(0,1) < explore_rate:\n",
    "        action = np.random.choice(actions_space)\n",
    "    ## exploit: select the action with max q_value\n",
    "    else:\n",
    "        current_qs = model.predict(current_state, verbose=0)[0]\n",
    "        action = np.argmax(current_qs)\n",
    "    return action\n",
    "\n",
    "\n",
    "def decay_exploration(explore_rate, min_eploration=0.01, explore_decay=0.99):\n",
    "    if explore_rate > min_eploration:\n",
    "        epsilon_decayed = explore_rate * explore_decay\n",
    "        explore_rate = max(min_eploration, epsilon_decayed)\n",
    "    return explore_rate\n",
    "\n",
    "\n",
    "def update_Q(model, memory, min_memory_size=1000, batch_size=64, discount_rate=0.8):\n",
    "    # start only if there are enough experiences\n",
    "    if len(memory) > min_memory_size:\n",
    "        ## get batch of random samples from the memory\n",
    "        batch = random.sample(memory, batch_size) \n",
    "        ## build dataset while replaying\n",
    "        lst_X = [] #store states\n",
    "        lst_y = [] #store actions's qs\n",
    "        ## replay\n",
    "        for current_state, action, reward, new_state, done in batch:\n",
    "            ### comupte new_qs\n",
    "            if not done:\n",
    "                new_qs = model.predict(new_state, verbose=0)[0]\n",
    "                max_q_value = np.max(new_qs)\n",
    "                new_q_value = reward + discount_rate * max_q_value\n",
    "            else:\n",
    "                new_q_value = reward\n",
    "            ### update current_qs\n",
    "            current_qs = model.predict(current_state, verbose=0)\n",
    "            current_qs[0][action] = new_q_value\n",
    "            ### dataset\n",
    "            lst_X.append(current_state[0])\n",
    "            lst_y.append(current_qs[0])\n",
    "        ## fit\n",
    "        X = np.array(lst_X)  #array: batch_size x states_space\n",
    "        y = np.array(lst_y)  #array: batch_size x actions_space\n",
    "        model.fit(x=X, y=y, batch_size=batch_size, epochs=1, verbose=0, shuffle=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = collections.deque(maxlen=1000) #<--rolling list\n",
    "explore_rate = 1\n",
    "episodes = 100\n",
    "render_every = episodes/4\n",
    "\n",
    "for n,episode in enumerate(tqdm(range(episodes))):\n",
    "    ## start episode\n",
    "    current_state = env.reset(from_start=True)\n",
    "    current_state = preprocessing(current_state)\n",
    "    done = False\n",
    "    while not done:\n",
    "        ## get next action\n",
    "        action = next_action(model, current_state, action_space, explore_rate=explore_rate)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        ## update model\n",
    "        new_state = preprocessing(new_state)\n",
    "        memory.append((current_state, action, reward, new_state, done))\n",
    "        model = update_Q(model, memory, min_memory_size=memory.maxlen/10, batch_size=64, discount_rate=0.8)\n",
    "        ## next\n",
    "        explore_rate = decay_exploration(explore_rate=explore_rate, min_eploration=0.001, explore_decay=0.99)\n",
    "        current_state = new_state\n",
    "    ## render episode\n",
    "    if n % render_every == 0:\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = env.reset(from_start=False)\n",
    "current_state = preprocessing(current_state)\n",
    "done = False\n",
    "while not done:\n",
    "    action = next_action(model, current_state, action_space, explore_rate=0.5)\n",
    "    new_state, reward, done, info = env.step(action)\n",
    "    new_state = preprocessing(new_state)\n",
    "    current_state = new_state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - A2C (continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "model = A2C(policy=\"CnnPolicy\", env=env, verbose=1, n_steps=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
